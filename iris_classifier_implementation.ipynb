{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 95\n",
      "Trainable params: 95\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/sayannath235/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "120/120 [==============================] - 2s 20ms/step - loss: 1.2131 - acc: 0.3333 - val_loss: 1.1349 - val_acc: 0.3333\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 208us/step - loss: 1.2044 - acc: 0.3333 - val_loss: 1.1318 - val_acc: 0.3333\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 1.1961 - acc: 0.3333 - val_loss: 1.1293 - val_acc: 0.3333\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 212us/step - loss: 1.1863 - acc: 0.3333 - val_loss: 1.1273 - val_acc: 0.3333\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 191us/step - loss: 1.1791 - acc: 0.3333 - val_loss: 1.1254 - val_acc: 0.3333\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 217us/step - loss: 1.1720 - acc: 0.3333 - val_loss: 1.1232 - val_acc: 0.3000\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 188us/step - loss: 1.1659 - acc: 0.3333 - val_loss: 1.1213 - val_acc: 0.3000\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 1.1590 - acc: 0.3333 - val_loss: 1.1197 - val_acc: 0.3000\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 197us/step - loss: 1.1535 - acc: 0.3333 - val_loss: 1.1182 - val_acc: 0.3000\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 1.1499 - acc: 0.3333 - val_loss: 1.1167 - val_acc: 0.3000\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 187us/step - loss: 1.1437 - acc: 0.3333 - val_loss: 1.1154 - val_acc: 0.2333\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 252us/step - loss: 1.1411 - acc: 0.3333 - val_loss: 1.1145 - val_acc: 0.2333\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 228us/step - loss: 1.1374 - acc: 0.2917 - val_loss: 1.1137 - val_acc: 0.2333\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 212us/step - loss: 1.1338 - acc: 0.2750 - val_loss: 1.1131 - val_acc: 0.2000\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 260us/step - loss: 1.1311 - acc: 0.2333 - val_loss: 1.1124 - val_acc: 0.2000\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 283us/step - loss: 1.1278 - acc: 0.2000 - val_loss: 1.1115 - val_acc: 0.2000\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 210us/step - loss: 1.1248 - acc: 0.1333 - val_loss: 1.1107 - val_acc: 0.1667\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 173us/step - loss: 1.1215 - acc: 0.1167 - val_loss: 1.1094 - val_acc: 0.1333\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 206us/step - loss: 1.1180 - acc: 0.1083 - val_loss: 1.1083 - val_acc: 0.2333\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 216us/step - loss: 1.1140 - acc: 0.1000 - val_loss: 1.1076 - val_acc: 0.2333\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 181us/step - loss: 1.1107 - acc: 0.0917 - val_loss: 1.1073 - val_acc: 0.2333\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 200us/step - loss: 1.1094 - acc: 0.1083 - val_loss: 1.1072 - val_acc: 0.2000\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 186us/step - loss: 1.1084 - acc: 0.1583 - val_loss: 1.1071 - val_acc: 0.2333\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 178us/step - loss: 1.1077 - acc: 0.2083 - val_loss: 1.1069 - val_acc: 0.2333\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 214us/step - loss: 1.1071 - acc: 0.2500 - val_loss: 1.1067 - val_acc: 0.2333\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 212us/step - loss: 1.1066 - acc: 0.2417 - val_loss: 1.1066 - val_acc: 0.2333\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 182us/step - loss: 1.1061 - acc: 0.2583 - val_loss: 1.1064 - val_acc: 0.2667\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 175us/step - loss: 1.1056 - acc: 0.2750 - val_loss: 1.1062 - val_acc: 0.2667\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 189us/step - loss: 1.1050 - acc: 0.3000 - val_loss: 1.1060 - val_acc: 0.2667\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 194us/step - loss: 1.1046 - acc: 0.3000 - val_loss: 1.1058 - val_acc: 0.2667\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 190us/step - loss: 1.1041 - acc: 0.3000 - val_loss: 1.1056 - val_acc: 0.2667\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 181us/step - loss: 1.1035 - acc: 0.3167 - val_loss: 1.1054 - val_acc: 0.2667\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 165us/step - loss: 1.1030 - acc: 0.3167 - val_loss: 1.1051 - val_acc: 0.2667\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 200us/step - loss: 1.1025 - acc: 0.3417 - val_loss: 1.1048 - val_acc: 0.2667\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 145us/step - loss: 1.1019 - acc: 0.3500 - val_loss: 1.1045 - val_acc: 0.2667\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 1.1014 - acc: 0.3500 - val_loss: 1.1043 - val_acc: 0.2667\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 184us/step - loss: 1.1008 - acc: 0.3500 - val_loss: 1.1043 - val_acc: 0.2667\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 162us/step - loss: 1.1003 - acc: 0.3500 - val_loss: 1.1042 - val_acc: 0.2667\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 171us/step - loss: 1.0998 - acc: 0.3500 - val_loss: 1.1041 - val_acc: 0.2667\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 197us/step - loss: 1.0993 - acc: 0.3500 - val_loss: 1.1040 - val_acc: 0.2667\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 189us/step - loss: 1.0988 - acc: 0.3500 - val_loss: 1.1039 - val_acc: 0.2667\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 228us/step - loss: 1.0984 - acc: 0.3500 - val_loss: 1.1038 - val_acc: 0.2667\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 141us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.1038 - val_acc: 0.2667\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 166us/step - loss: 1.0979 - acc: 0.3500 - val_loss: 1.1038 - val_acc: 0.2667\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 194us/step - loss: 1.0977 - acc: 0.3500 - val_loss: 1.1037 - val_acc: 0.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 164us/step - loss: 1.0974 - acc: 0.3500 - val_loss: 1.1036 - val_acc: 0.2667\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 168us/step - loss: 1.0971 - acc: 0.3500 - val_loss: 1.1033 - val_acc: 0.2667\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 178us/step - loss: 1.0967 - acc: 0.3500 - val_loss: 1.1029 - val_acc: 0.2667\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 1.0963 - acc: 0.3500 - val_loss: 1.1023 - val_acc: 0.3000\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 160us/step - loss: 1.0957 - acc: 0.3583 - val_loss: 1.1016 - val_acc: 0.3000\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 166us/step - loss: 1.0951 - acc: 0.3750 - val_loss: 1.1005 - val_acc: 0.3333\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 168us/step - loss: 1.0940 - acc: 0.4000 - val_loss: 1.0990 - val_acc: 0.3667\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 162us/step - loss: 1.0925 - acc: 0.4250 - val_loss: 1.0974 - val_acc: 0.4667\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 164us/step - loss: 1.0909 - acc: 0.4833 - val_loss: 1.0960 - val_acc: 0.4667\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 1.0901 - acc: 0.5250 - val_loss: 1.0951 - val_acc: 0.4667\n",
      "Epoch 56/300\n",
      "120/120 [==============================] - 0s 166us/step - loss: 1.0890 - acc: 0.5500 - val_loss: 1.0940 - val_acc: 0.4667\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 139us/step - loss: 1.0875 - acc: 0.5833 - val_loss: 1.0927 - val_acc: 0.4667\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 152us/step - loss: 1.0860 - acc: 0.6167 - val_loss: 1.0913 - val_acc: 0.5000\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 176us/step - loss: 1.0842 - acc: 0.6500 - val_loss: 1.0900 - val_acc: 0.5000\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 164us/step - loss: 1.0824 - acc: 0.6750 - val_loss: 1.0886 - val_acc: 0.5000\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 149us/step - loss: 1.0806 - acc: 0.6833 - val_loss: 1.0869 - val_acc: 0.5333\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 136us/step - loss: 1.0786 - acc: 0.6833 - val_loss: 1.0848 - val_acc: 0.5667\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 144us/step - loss: 1.0764 - acc: 0.6833 - val_loss: 1.0828 - val_acc: 0.5667\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 1.0741 - acc: 0.6833 - val_loss: 1.0805 - val_acc: 0.5667\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 168us/step - loss: 1.0717 - acc: 0.6833 - val_loss: 1.0781 - val_acc: 0.5667\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 162us/step - loss: 1.0690 - acc: 0.6833 - val_loss: 1.0754 - val_acc: 0.5667\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 1.0661 - acc: 0.6833 - val_loss: 1.0723 - val_acc: 0.5667\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 159us/step - loss: 1.0627 - acc: 0.6833 - val_loss: 1.0689 - val_acc: 0.5667\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 1.0595 - acc: 0.6750 - val_loss: 1.0650 - val_acc: 0.5667\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 144us/step - loss: 1.0557 - acc: 0.6667 - val_loss: 1.0610 - val_acc: 0.6000\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 143us/step - loss: 1.0518 - acc: 0.6667 - val_loss: 1.0568 - val_acc: 0.6000\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 156us/step - loss: 1.0476 - acc: 0.6583 - val_loss: 1.0526 - val_acc: 0.6000\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 169us/step - loss: 1.0432 - acc: 0.6583 - val_loss: 1.0479 - val_acc: 0.6000\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 180us/step - loss: 1.0384 - acc: 0.6583 - val_loss: 1.0432 - val_acc: 0.6000\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 1.0335 - acc: 0.6583 - val_loss: 1.0379 - val_acc: 0.6000\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 174us/step - loss: 1.0284 - acc: 0.6583 - val_loss: 1.0328 - val_acc: 0.6000\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 1.0228 - acc: 0.6583 - val_loss: 1.0280 - val_acc: 0.6000\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 167us/step - loss: 1.0170 - acc: 0.6583 - val_loss: 1.0220 - val_acc: 0.6000\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 152us/step - loss: 1.0109 - acc: 0.6583 - val_loss: 1.0156 - val_acc: 0.6000\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 135us/step - loss: 1.0046 - acc: 0.6583 - val_loss: 1.0094 - val_acc: 0.6000\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.9974 - acc: 0.6583 - val_loss: 1.0027 - val_acc: 0.6000\n",
      "Epoch 82/300\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.9909 - acc: 0.6583 - val_loss: 0.9954 - val_acc: 0.6000\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.9827 - acc: 0.6667 - val_loss: 0.9884 - val_acc: 0.6000\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.9757 - acc: 0.6667 - val_loss: 0.9808 - val_acc: 0.6000\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.9676 - acc: 0.6667 - val_loss: 0.9730 - val_acc: 0.6000\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.9590 - acc: 0.6667 - val_loss: 0.9650 - val_acc: 0.6000\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.9501 - acc: 0.6667 - val_loss: 0.9567 - val_acc: 0.6000\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.9420 - acc: 0.6667 - val_loss: 0.9480 - val_acc: 0.6000\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.9330 - acc: 0.6667 - val_loss: 0.9395 - val_acc: 0.6000\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.9240 - acc: 0.6667 - val_loss: 0.9306 - val_acc: 0.6000\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.9143 - acc: 0.6667 - val_loss: 0.9218 - val_acc: 0.6000\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.9048 - acc: 0.6667 - val_loss: 0.9130 - val_acc: 0.6000\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.8953 - acc: 0.6750 - val_loss: 0.9044 - val_acc: 0.6000\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.8857 - acc: 0.6750 - val_loss: 0.8956 - val_acc: 0.6000\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.8765 - acc: 0.6750 - val_loss: 0.8868 - val_acc: 0.6000\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.8668 - acc: 0.6833 - val_loss: 0.8783 - val_acc: 0.6000\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.8576 - acc: 0.6833 - val_loss: 0.8691 - val_acc: 0.6000\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.8484 - acc: 0.6833 - val_loss: 0.8606 - val_acc: 0.6000\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.8389 - acc: 0.6833 - val_loss: 0.8524 - val_acc: 0.6000\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.8304 - acc: 0.6833 - val_loss: 0.8440 - val_acc: 0.6000\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.8215 - acc: 0.6833 - val_loss: 0.8361 - val_acc: 0.6000\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.8133 - acc: 0.6833 - val_loss: 0.8283 - val_acc: 0.6000\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.8050 - acc: 0.6833 - val_loss: 0.8210 - val_acc: 0.6000\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.7970 - acc: 0.6833 - val_loss: 0.8138 - val_acc: 0.6000\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.7897 - acc: 0.6833 - val_loss: 0.8070 - val_acc: 0.6000\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.7827 - acc: 0.6833 - val_loss: 0.8003 - val_acc: 0.6000\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.7756 - acc: 0.6833 - val_loss: 0.7940 - val_acc: 0.6000\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 203us/step - loss: 0.7688 - acc: 0.6833 - val_loss: 0.7879 - val_acc: 0.6000\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.7625 - acc: 0.6833 - val_loss: 0.7821 - val_acc: 0.6000\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.7564 - acc: 0.6833 - val_loss: 0.7766 - val_acc: 0.6000\n",
      "Epoch 111/300\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.7506 - acc: 0.6833 - val_loss: 0.7712 - val_acc: 0.6000\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.7451 - acc: 0.6833 - val_loss: 0.7662 - val_acc: 0.6000\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 244us/step - loss: 0.7399 - acc: 0.6833 - val_loss: 0.7610 - val_acc: 0.6000\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.7347 - acc: 0.6833 - val_loss: 0.7564 - val_acc: 0.6000\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.7298 - acc: 0.6833 - val_loss: 0.7517 - val_acc: 0.6000\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.7252 - acc: 0.6833 - val_loss: 0.7473 - val_acc: 0.6000\n",
      "Epoch 117/300\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.7206 - acc: 0.6833 - val_loss: 0.7429 - val_acc: 0.6000\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.7163 - acc: 0.6833 - val_loss: 0.7389 - val_acc: 0.6000\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.7121 - acc: 0.6833 - val_loss: 0.7352 - val_acc: 0.6000\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.7082 - acc: 0.6833 - val_loss: 0.7313 - val_acc: 0.6000\n",
      "Epoch 121/300\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.7044 - acc: 0.6833 - val_loss: 0.7276 - val_acc: 0.6000\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.7008 - acc: 0.6833 - val_loss: 0.7242 - val_acc: 0.6000\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.6973 - acc: 0.6833 - val_loss: 0.7209 - val_acc: 0.6000\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.6940 - acc: 0.6833 - val_loss: 0.7175 - val_acc: 0.6000\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.6907 - acc: 0.6833 - val_loss: 0.7143 - val_acc: 0.6000\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.6877 - acc: 0.6833 - val_loss: 0.7114 - val_acc: 0.6000\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.6848 - acc: 0.6833 - val_loss: 0.7083 - val_acc: 0.6000\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.6818 - acc: 0.6833 - val_loss: 0.7054 - val_acc: 0.6000\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.6790 - acc: 0.6833 - val_loss: 0.7026 - val_acc: 0.6000\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6764 - acc: 0.6833 - val_loss: 0.7000 - val_acc: 0.6000\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.6738 - acc: 0.6833 - val_loss: 0.6973 - val_acc: 0.6000\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.6712 - acc: 0.6833 - val_loss: 0.6946 - val_acc: 0.6000\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.6686 - acc: 0.7083 - val_loss: 0.6923 - val_acc: 0.6000\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.6663 - acc: 0.7083 - val_loss: 0.6897 - val_acc: 0.6333\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.6640 - acc: 0.6917 - val_loss: 0.6873 - val_acc: 0.6333\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6617 - acc: 0.7000 - val_loss: 0.6850 - val_acc: 0.6333\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.6594 - acc: 0.7083 - val_loss: 0.6826 - val_acc: 0.6333\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.6572 - acc: 0.7083 - val_loss: 0.6803 - val_acc: 0.6333\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.6551 - acc: 0.7083 - val_loss: 0.6781 - val_acc: 0.6333\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.6532 - acc: 0.7083 - val_loss: 0.6759 - val_acc: 0.6333\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.6512 - acc: 0.7083 - val_loss: 0.6737 - val_acc: 0.6333\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.6492 - acc: 0.7083 - val_loss: 0.6716 - val_acc: 0.6333\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.6471 - acc: 0.7083 - val_loss: 0.6696 - val_acc: 0.6667\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6452 - acc: 0.7083 - val_loss: 0.6675 - val_acc: 0.6667\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.6434 - acc: 0.7083 - val_loss: 0.6655 - val_acc: 0.6667\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.6415 - acc: 0.7083 - val_loss: 0.6635 - val_acc: 0.6667\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.6396 - acc: 0.7083 - val_loss: 0.6615 - val_acc: 0.6667\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.6378 - acc: 0.7167 - val_loss: 0.6595 - val_acc: 0.6667\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.6361 - acc: 0.7083 - val_loss: 0.6576 - val_acc: 0.6667\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.6343 - acc: 0.7250 - val_loss: 0.6556 - val_acc: 0.6667\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.6324 - acc: 0.7250 - val_loss: 0.6537 - val_acc: 0.6667\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.6306 - acc: 0.7333 - val_loss: 0.6518 - val_acc: 0.6667\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.6289 - acc: 0.7333 - val_loss: 0.6499 - val_acc: 0.6667\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.6273 - acc: 0.7500 - val_loss: 0.6482 - val_acc: 0.6667\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.6258 - acc: 0.7583 - val_loss: 0.6465 - val_acc: 0.6667\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.6241 - acc: 0.7583 - val_loss: 0.6445 - val_acc: 0.6667\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.6222 - acc: 0.7583 - val_loss: 0.6425 - val_acc: 0.6667\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.6203 - acc: 0.7667 - val_loss: 0.6402 - val_acc: 0.6667\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.6186 - acc: 0.7833 - val_loss: 0.6382 - val_acc: 0.6667\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.6168 - acc: 0.7833 - val_loss: 0.6364 - val_acc: 0.7000\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.6152 - acc: 0.7917 - val_loss: 0.6344 - val_acc: 0.7000\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.6135 - acc: 0.8000 - val_loss: 0.6322 - val_acc: 0.7000\n",
      "Epoch 163/300\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.6115 - acc: 0.8083 - val_loss: 0.6302 - val_acc: 0.6667\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.6099 - acc: 0.8333 - val_loss: 0.6279 - val_acc: 0.7000\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.6081 - acc: 0.8333 - val_loss: 0.6259 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/300\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.6064 - acc: 0.8333 - val_loss: 0.6239 - val_acc: 0.7000\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.6048 - acc: 0.8417 - val_loss: 0.6218 - val_acc: 0.7333\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6031 - acc: 0.8500 - val_loss: 0.6195 - val_acc: 0.8000\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.6013 - acc: 0.8500 - val_loss: 0.6173 - val_acc: 0.8333\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5996 - acc: 0.8500 - val_loss: 0.6152 - val_acc: 0.8333\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.5979 - acc: 0.8500 - val_loss: 0.6132 - val_acc: 0.8333\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.5961 - acc: 0.8583 - val_loss: 0.6114 - val_acc: 0.8667\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5946 - acc: 0.8583 - val_loss: 0.6094 - val_acc: 0.8667\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.5929 - acc: 0.8583 - val_loss: 0.6072 - val_acc: 0.9000\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.5911 - acc: 0.8583 - val_loss: 0.6051 - val_acc: 0.9000\n",
      "Epoch 176/300\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.5896 - acc: 0.8750 - val_loss: 0.6031 - val_acc: 0.9000\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.5876 - acc: 0.8833 - val_loss: 0.6011 - val_acc: 0.9000\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5859 - acc: 0.8750 - val_loss: 0.5990 - val_acc: 0.9000\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.5842 - acc: 0.8833 - val_loss: 0.5967 - val_acc: 0.9333\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5824 - acc: 0.9000 - val_loss: 0.5944 - val_acc: 0.9333\n",
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.5804 - acc: 0.9000 - val_loss: 0.5921 - val_acc: 0.9333\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5787 - acc: 0.9000 - val_loss: 0.5900 - val_acc: 0.9333\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.5770 - acc: 0.9083 - val_loss: 0.5877 - val_acc: 0.9333\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5751 - acc: 0.9083 - val_loss: 0.5854 - val_acc: 0.9333\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5732 - acc: 0.9167 - val_loss: 0.5830 - val_acc: 0.9333\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5715 - acc: 0.9167 - val_loss: 0.5805 - val_acc: 0.9333\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5696 - acc: 0.9250 - val_loss: 0.5786 - val_acc: 0.9333\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.5677 - acc: 0.9250 - val_loss: 0.5765 - val_acc: 0.9333\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5661 - acc: 0.9250 - val_loss: 0.5744 - val_acc: 0.9333\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5641 - acc: 0.9333 - val_loss: 0.5714 - val_acc: 0.9333\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.5621 - acc: 0.9250 - val_loss: 0.5686 - val_acc: 0.9333\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.5601 - acc: 0.9250 - val_loss: 0.5661 - val_acc: 0.9333\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5581 - acc: 0.9250 - val_loss: 0.5642 - val_acc: 0.9333\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5565 - acc: 0.9250 - val_loss: 0.5616 - val_acc: 0.9333\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5545 - acc: 0.9250 - val_loss: 0.5593 - val_acc: 0.9333\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5522 - acc: 0.9333 - val_loss: 0.5560 - val_acc: 0.9333\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5503 - acc: 0.9250 - val_loss: 0.5536 - val_acc: 0.9333\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5482 - acc: 0.9250 - val_loss: 0.5512 - val_acc: 0.9333\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5461 - acc: 0.9333 - val_loss: 0.5484 - val_acc: 0.9000\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5442 - acc: 0.9250 - val_loss: 0.5456 - val_acc: 0.9000\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5422 - acc: 0.9250 - val_loss: 0.5431 - val_acc: 0.9000\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5399 - acc: 0.9250 - val_loss: 0.5409 - val_acc: 0.9000\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5379 - acc: 0.9250 - val_loss: 0.5386 - val_acc: 0.9000\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.5360 - acc: 0.9250 - val_loss: 0.5358 - val_acc: 0.9000\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5338 - acc: 0.9250 - val_loss: 0.5332 - val_acc: 0.9000\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5319 - acc: 0.9250 - val_loss: 0.5306 - val_acc: 0.9000\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5298 - acc: 0.9250 - val_loss: 0.5286 - val_acc: 0.9000\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5278 - acc: 0.9250 - val_loss: 0.5254 - val_acc: 0.9000\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5257 - acc: 0.9167 - val_loss: 0.5227 - val_acc: 0.9333\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5237 - acc: 0.9167 - val_loss: 0.5205 - val_acc: 0.9000\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5213 - acc: 0.9250 - val_loss: 0.5175 - val_acc: 0.9333\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.5194 - acc: 0.9167 - val_loss: 0.5147 - val_acc: 0.9333\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5172 - acc: 0.9167 - val_loss: 0.5126 - val_acc: 0.9333\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5151 - acc: 0.9250 - val_loss: 0.5097 - val_acc: 0.9333\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5130 - acc: 0.9167 - val_loss: 0.5064 - val_acc: 0.9333\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5107 - acc: 0.9250 - val_loss: 0.5040 - val_acc: 0.9333\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5087 - acc: 0.9250 - val_loss: 0.5014 - val_acc: 0.9333\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5067 - acc: 0.9250 - val_loss: 0.4987 - val_acc: 0.9333\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5048 - acc: 0.9167 - val_loss: 0.4973 - val_acc: 0.9333\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.5022 - acc: 0.9167 - val_loss: 0.4933 - val_acc: 0.9333\n",
      "Epoch 221/300\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4998 - acc: 0.9333 - val_loss: 0.4906 - val_acc: 0.9333\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4977 - acc: 0.9333 - val_loss: 0.4880 - val_acc: 0.9333\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4955 - acc: 0.9333 - val_loss: 0.4853 - val_acc: 0.9333\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4934 - acc: 0.9333 - val_loss: 0.4828 - val_acc: 0.9333\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4913 - acc: 0.9333 - val_loss: 0.4797 - val_acc: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4891 - acc: 0.9333 - val_loss: 0.4772 - val_acc: 0.9333\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4869 - acc: 0.9333 - val_loss: 0.4755 - val_acc: 0.9333\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4849 - acc: 0.9333 - val_loss: 0.4714 - val_acc: 0.9333\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4828 - acc: 0.9167 - val_loss: 0.4688 - val_acc: 0.9333\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4803 - acc: 0.9333 - val_loss: 0.4674 - val_acc: 0.9333\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4782 - acc: 0.9333 - val_loss: 0.4636 - val_acc: 0.9333\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4759 - acc: 0.9333 - val_loss: 0.4610 - val_acc: 0.9333\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4736 - acc: 0.9250 - val_loss: 0.4577 - val_acc: 0.9333\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4715 - acc: 0.9250 - val_loss: 0.4545 - val_acc: 0.9333\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4696 - acc: 0.9333 - val_loss: 0.4540 - val_acc: 0.9333\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4671 - acc: 0.9333 - val_loss: 0.4509 - val_acc: 0.9333\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4647 - acc: 0.9333 - val_loss: 0.4455 - val_acc: 0.9333\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.4625 - acc: 0.9333 - val_loss: 0.4424 - val_acc: 0.9333\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4598 - acc: 0.9333 - val_loss: 0.4406 - val_acc: 0.9333\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4575 - acc: 0.9333 - val_loss: 0.4385 - val_acc: 0.9333\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4555 - acc: 0.9333 - val_loss: 0.4351 - val_acc: 0.9333\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4531 - acc: 0.9417 - val_loss: 0.4324 - val_acc: 0.9333\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4508 - acc: 0.9250 - val_loss: 0.4297 - val_acc: 0.9333\n",
      "Epoch 244/300\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4487 - acc: 0.9333 - val_loss: 0.4268 - val_acc: 0.9333\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4459 - acc: 0.9333 - val_loss: 0.4248 - val_acc: 0.9333\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4439 - acc: 0.9417 - val_loss: 0.4225 - val_acc: 0.9333\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4421 - acc: 0.9333 - val_loss: 0.4186 - val_acc: 0.9333\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4394 - acc: 0.9333 - val_loss: 0.4156 - val_acc: 0.9333\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4370 - acc: 0.9333 - val_loss: 0.4131 - val_acc: 0.9333\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4348 - acc: 0.9333 - val_loss: 0.4105 - val_acc: 0.9333\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4325 - acc: 0.9333 - val_loss: 0.4077 - val_acc: 0.9333\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4302 - acc: 0.9333 - val_loss: 0.4052 - val_acc: 0.9333\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4282 - acc: 0.9333 - val_loss: 0.4026 - val_acc: 0.9333\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4257 - acc: 0.9333 - val_loss: 0.4006 - val_acc: 0.9333\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4239 - acc: 0.9333 - val_loss: 0.3975 - val_acc: 0.9333\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.4215 - acc: 0.9417 - val_loss: 0.3954 - val_acc: 0.9333\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4193 - acc: 0.9333 - val_loss: 0.3924 - val_acc: 0.9333\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4169 - acc: 0.9333 - val_loss: 0.3896 - val_acc: 0.9333\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4146 - acc: 0.9333 - val_loss: 0.3870 - val_acc: 0.9333\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4124 - acc: 0.9333 - val_loss: 0.3843 - val_acc: 0.9333\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4101 - acc: 0.9333 - val_loss: 0.3820 - val_acc: 0.9333\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4078 - acc: 0.9333 - val_loss: 0.3798 - val_acc: 0.9333\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 216us/step - loss: 0.4057 - acc: 0.9333 - val_loss: 0.3772 - val_acc: 0.9333\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4038 - acc: 0.9333 - val_loss: 0.3743 - val_acc: 0.9333\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4014 - acc: 0.9333 - val_loss: 0.3720 - val_acc: 0.9333\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3993 - acc: 0.9333 - val_loss: 0.3698 - val_acc: 0.9333\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3971 - acc: 0.9333 - val_loss: 0.3672 - val_acc: 0.9333\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3946 - acc: 0.9333 - val_loss: 0.3648 - val_acc: 0.9333\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3927 - acc: 0.9417 - val_loss: 0.3630 - val_acc: 0.9333\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3925 - acc: 0.9333 - val_loss: 0.3599 - val_acc: 0.9333\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.3879 - acc: 0.9417 - val_loss: 0.3580 - val_acc: 0.9333\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3863 - acc: 0.9417 - val_loss: 0.3559 - val_acc: 0.9333\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3846 - acc: 0.9417 - val_loss: 0.3529 - val_acc: 0.9333\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3823 - acc: 0.9333 - val_loss: 0.3509 - val_acc: 0.9333\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3804 - acc: 0.9417 - val_loss: 0.3488 - val_acc: 0.9333\n",
      "Epoch 276/300\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3786 - acc: 0.9417 - val_loss: 0.3462 - val_acc: 0.9333\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3759 - acc: 0.9417 - val_loss: 0.3436 - val_acc: 0.9333\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3736 - acc: 0.9417 - val_loss: 0.3411 - val_acc: 0.9333\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3713 - acc: 0.9417 - val_loss: 0.3386 - val_acc: 0.9333\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3705 - acc: 0.9333 - val_loss: 0.3360 - val_acc: 0.9333\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3677 - acc: 0.9333 - val_loss: 0.3350 - val_acc: 0.9333\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3652 - acc: 0.9417 - val_loss: 0.3328 - val_acc: 0.9333\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3630 - acc: 0.9417 - val_loss: 0.3301 - val_acc: 0.9333\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3622 - acc: 0.9333 - val_loss: 0.3273 - val_acc: 0.9333\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3591 - acc: 0.9333 - val_loss: 0.3253 - val_acc: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3567 - acc: 0.9417 - val_loss: 0.3240 - val_acc: 0.9333\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3550 - acc: 0.9417 - val_loss: 0.3223 - val_acc: 0.9333\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3533 - acc: 0.9417 - val_loss: 0.3200 - val_acc: 0.9333\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3510 - acc: 0.9417 - val_loss: 0.3170 - val_acc: 0.9333\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3491 - acc: 0.9417 - val_loss: 0.3146 - val_acc: 0.9333\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3469 - acc: 0.9417 - val_loss: 0.3127 - val_acc: 0.9333\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3449 - acc: 0.9417 - val_loss: 0.3109 - val_acc: 0.9333\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3430 - acc: 0.9417 - val_loss: 0.3088 - val_acc: 0.9333\n",
      "Epoch 294/300\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3421 - acc: 0.9417 - val_loss: 0.3075 - val_acc: 0.9333\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3392 - acc: 0.9417 - val_loss: 0.3045 - val_acc: 0.9333\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3372 - acc: 0.9417 - val_loss: 0.3024 - val_acc: 0.9333\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3353 - acc: 0.9417 - val_loss: 0.3005 - val_acc: 0.9333\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3335 - acc: 0.9417 - val_loss: 0.2994 - val_acc: 0.9333\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3321 - acc: 0.9417 - val_loss: 0.2979 - val_acc: 0.9333\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3298 - acc: 0.9417 - val_loss: 0.2949 - val_acc: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8ca8559d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_X_train, y=y_train, epochs=300, validation_data=(scaled_X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa8ca87c810>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29490435123443604, 0.9333333373069763]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaled_X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "epochs = len(metrics)\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "model.add(Dense(units=4, activation='relu', input_shape=[4,]))\n",
    "\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 1.1028 - acc: 0.3333\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 0s 168us/step - loss: 1.0981 - acc: 0.3267\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 0s 190us/step - loss: 1.0945 - acc: 0.3867\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 0s 154us/step - loss: 1.0910 - acc: 0.4467\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 0s 155us/step - loss: 1.0883 - acc: 0.4800\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 0s 149us/step - loss: 1.0854 - acc: 0.5333\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 0s 137us/step - loss: 1.0823 - acc: 0.5467\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 0s 152us/step - loss: 1.0788 - acc: 0.5467\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 0s 194us/step - loss: 1.0753 - acc: 0.5467\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 0s 169us/step - loss: 1.0718 - acc: 0.5467\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 0s 142us/step - loss: 1.0679 - acc: 0.5467\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 0s 147us/step - loss: 1.0634 - acc: 0.5533\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 0s 168us/step - loss: 1.0591 - acc: 0.5733\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 0s 172us/step - loss: 1.0541 - acc: 0.6000\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 0s 183us/step - loss: 1.0489 - acc: 0.6133\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 0s 170us/step - loss: 1.0430 - acc: 0.6067\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 0s 175us/step - loss: 1.0369 - acc: 0.6067\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 0s 175us/step - loss: 1.0304 - acc: 0.5933\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 0s 168us/step - loss: 1.0231 - acc: 0.6067\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 0s 186us/step - loss: 1.0144 - acc: 0.6200\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 0s 201us/step - loss: 1.0055 - acc: 0.6267\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 0s 171us/step - loss: 0.9948 - acc: 0.6267\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 0s 171us/step - loss: 0.9825 - acc: 0.6200\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 0s 174us/step - loss: 0.9669 - acc: 0.6200\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 0s 163us/step - loss: 0.9488 - acc: 0.6267\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 0s 156us/step - loss: 0.9258 - acc: 0.6333\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 0s 143us/step - loss: 0.8997 - acc: 0.6467\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.8756 - acc: 0.6533\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.8516 - acc: 0.6600\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 0s 174us/step - loss: 0.8298 - acc: 0.6600\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 0s 205us/step - loss: 0.8111 - acc: 0.6600\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.7942 - acc: 0.6533\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 0s 171us/step - loss: 0.7782 - acc: 0.6533\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 0s 138us/step - loss: 0.7650 - acc: 0.6533\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.7518 - acc: 0.6533\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 0s 160us/step - loss: 0.7402 - acc: 0.6533\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 0s 137us/step - loss: 0.7290 - acc: 0.6600\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 0s 171us/step - loss: 0.7182 - acc: 0.6533\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 0s 171us/step - loss: 0.7074 - acc: 0.6533\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 0s 161us/step - loss: 0.6960 - acc: 0.6600\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 0s 177us/step - loss: 0.6858 - acc: 0.6600\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 0s 167us/step - loss: 0.6770 - acc: 0.6600\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 0s 150us/step - loss: 0.6693 - acc: 0.6600\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 0s 161us/step - loss: 0.6628 - acc: 0.6600\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 0s 161us/step - loss: 0.6569 - acc: 0.6600\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - 0s 143us/step - loss: 0.6516 - acc: 0.6600\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 0s 165us/step - loss: 0.6462 - acc: 0.6600\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 0s 186us/step - loss: 0.6412 - acc: 0.6600\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 0s 185us/step - loss: 0.6366 - acc: 0.6600\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 0s 163us/step - loss: 0.6320 - acc: 0.6600\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 0s 157us/step - loss: 0.6276 - acc: 0.6600\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 0s 174us/step - loss: 0.6234 - acc: 0.6600\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 0s 147us/step - loss: 0.6195 - acc: 0.6600\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 0s 134us/step - loss: 0.6156 - acc: 0.6600\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 0s 178us/step - loss: 0.6120 - acc: 0.6600\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 0s 196us/step - loss: 0.6082 - acc: 0.6600\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 0s 167us/step - loss: 0.6048 - acc: 0.6600\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 0s 153us/step - loss: 0.6014 - acc: 0.6600\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 0s 147us/step - loss: 0.5985 - acc: 0.6600\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 0s 133us/step - loss: 0.5951 - acc: 0.6600\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 0s 142us/step - loss: 0.5921 - acc: 0.6600\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 0s 159us/step - loss: 0.5892 - acc: 0.6600\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 0s 159us/step - loss: 0.5861 - acc: 0.6600\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 0s 170us/step - loss: 0.5834 - acc: 0.6667\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 0s 167us/step - loss: 0.5805 - acc: 0.6667\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 0s 162us/step - loss: 0.5778 - acc: 0.6600\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 0s 184us/step - loss: 0.5750 - acc: 0.6600\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 0s 146us/step - loss: 0.5725 - acc: 0.6667\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 0s 171us/step - loss: 0.5699 - acc: 0.6667\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 0s 153us/step - loss: 0.5675 - acc: 0.6600\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 0s 164us/step - loss: 0.5652 - acc: 0.6600\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 0s 162us/step - loss: 0.5628 - acc: 0.6667\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 0s 174us/step - loss: 0.5605 - acc: 0.6667\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 0s 162us/step - loss: 0.5581 - acc: 0.6667\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 0s 171us/step - loss: 0.5559 - acc: 0.6600\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 0s 161us/step - loss: 0.5538 - acc: 0.6600\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 0s 140us/step - loss: 0.5516 - acc: 0.6600\n",
      "Epoch 78/300\n",
      "150/150 [==============================] - 0s 178us/step - loss: 0.5496 - acc: 0.6600\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 0s 149us/step - loss: 0.5477 - acc: 0.6600\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 0s 130us/step - loss: 0.5456 - acc: 0.6600\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - 0s 148us/step - loss: 0.5437 - acc: 0.6600\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 0s 127us/step - loss: 0.5418 - acc: 0.6600\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 0s 144us/step - loss: 0.5401 - acc: 0.6600\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 0s 133us/step - loss: 0.5380 - acc: 0.6600\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 0s 149us/step - loss: 0.5363 - acc: 0.6600\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 0s 122us/step - loss: 0.5347 - acc: 0.6600\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 0s 121us/step - loss: 0.5328 - acc: 0.6600\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 0s 129us/step - loss: 0.5312 - acc: 0.6600\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 0s 150us/step - loss: 0.5297 - acc: 0.6600\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 0s 122us/step - loss: 0.5280 - acc: 0.6667\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 0s 117us/step - loss: 0.5266 - acc: 0.6667\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 0s 123us/step - loss: 0.5250 - acc: 0.6667\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 0s 120us/step - loss: 0.5236 - acc: 0.6667\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 0s 129us/step - loss: 0.5224 - acc: 0.6600\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 0s 127us/step - loss: 0.5207 - acc: 0.6600\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 0s 134us/step - loss: 0.5194 - acc: 0.6600\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 0s 123us/step - loss: 0.5180 - acc: 0.6600\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 0s 127us/step - loss: 0.5168 - acc: 0.6600\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 0s 139us/step - loss: 0.5154 - acc: 0.6600\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 0s 133us/step - loss: 0.5141 - acc: 0.6600\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 0s 126us/step - loss: 0.5128 - acc: 0.6600\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 0s 131us/step - loss: 0.5117 - acc: 0.6600\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 0s 156us/step - loss: 0.5105 - acc: 0.6667\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 0s 128us/step - loss: 0.5093 - acc: 0.6667\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 0s 126us/step - loss: 0.5082 - acc: 0.6667\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 0s 130us/step - loss: 0.5071 - acc: 0.6667\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 0s 154us/step - loss: 0.5060 - acc: 0.6600\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 0s 134us/step - loss: 0.5050 - acc: 0.6600\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 0s 122us/step - loss: 0.5040 - acc: 0.6600\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 0s 128us/step - loss: 0.5030 - acc: 0.6600\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 0s 122us/step - loss: 0.5021 - acc: 0.6600\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 0s 133us/step - loss: 0.5012 - acc: 0.6600\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 0s 145us/step - loss: 0.5003 - acc: 0.6600\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 0s 115us/step - loss: 0.4995 - acc: 0.6600\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - 0s 135us/step - loss: 0.4985 - acc: 0.6600\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 0s 124us/step - loss: 0.4977 - acc: 0.6600\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 0s 153us/step - loss: 0.4970 - acc: 0.6600\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - 0s 181us/step - loss: 0.4961 - acc: 0.6600\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 0s 131us/step - loss: 0.4953 - acc: 0.6600\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 0s 125us/step - loss: 0.4945 - acc: 0.6533\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 0s 124us/step - loss: 0.4938 - acc: 0.6533\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - 0s 132us/step - loss: 0.4930 - acc: 0.6533\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 0s 129us/step - loss: 0.4925 - acc: 0.6400\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 0s 129us/step - loss: 0.4919 - acc: 0.6400\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 0s 138us/step - loss: 0.4910 - acc: 0.6400\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 0s 123us/step - loss: 0.4906 - acc: 0.6400\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 0s 141us/step - loss: 0.4898 - acc: 0.6400\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.4892 - acc: 0.6400\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 0s 140us/step - loss: 0.4887 - acc: 0.6400\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - 0s 149us/step - loss: 0.4880 - acc: 0.6333\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 0s 140us/step - loss: 0.4875 - acc: 0.6333\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 0s 138us/step - loss: 0.4870 - acc: 0.6400\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 0s 160us/step - loss: 0.4864 - acc: 0.6333\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 0s 142us/step - loss: 0.4859 - acc: 0.6333\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 0s 173us/step - loss: 0.4854 - acc: 0.6267\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 0s 134us/step - loss: 0.4849 - acc: 0.6200\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - 0s 136us/step - loss: 0.4844 - acc: 0.6200\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 0s 134us/step - loss: 0.4840 - acc: 0.6200\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 0s 133us/step - loss: 0.4835 - acc: 0.6067\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 0s 126us/step - loss: 0.4830 - acc: 0.6067\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 0s 131us/step - loss: 0.4826 - acc: 0.6133\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 0s 136us/step - loss: 0.4822 - acc: 0.6333\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 0s 143us/step - loss: 0.4817 - acc: 0.6267\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 0s 131us/step - loss: 0.4814 - acc: 0.6267\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 0s 129us/step - loss: 0.4810 - acc: 0.6333\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 0s 153us/step - loss: 0.4805 - acc: 0.6333\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.4802 - acc: 0.6333\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 0s 118us/step - loss: 0.4798 - acc: 0.6533\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 0s 157us/step - loss: 0.4794 - acc: 0.6400\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 0s 145us/step - loss: 0.4791 - acc: 0.6467\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 0s 122us/step - loss: 0.4787 - acc: 0.6533\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 0s 163us/step - loss: 0.4784 - acc: 0.6467\n",
      "Epoch 153/300\n",
      "150/150 [==============================] - 0s 160us/step - loss: 0.4781 - acc: 0.6533\n",
      "Epoch 154/300\n",
      "150/150 [==============================] - 0s 150us/step - loss: 0.4777 - acc: 0.6600\n",
      "Epoch 155/300\n",
      "150/150 [==============================] - 0s 127us/step - loss: 0.4773 - acc: 0.6600\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 0s 142us/step - loss: 0.4770 - acc: 0.6600\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 0s 138us/step - loss: 0.4767 - acc: 0.6533\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 0s 164us/step - loss: 0.4765 - acc: 0.6533\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 0s 182us/step - loss: 0.4761 - acc: 0.6533\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 0s 144us/step - loss: 0.4758 - acc: 0.6667\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 0s 178us/step - loss: 0.4755 - acc: 0.6733\n",
      "Epoch 162/300\n",
      "150/150 [==============================] - 0s 154us/step - loss: 0.4752 - acc: 0.6733\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.4748 - acc: 0.6800\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 0s 169us/step - loss: 0.4746 - acc: 0.6733\n",
      "Epoch 165/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 162us/step - loss: 0.4742 - acc: 0.6800\n",
      "Epoch 166/300\n",
      "150/150 [==============================] - 0s 148us/step - loss: 0.4740 - acc: 0.6867\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 0s 127us/step - loss: 0.4737 - acc: 0.6867\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 0s 138us/step - loss: 0.4734 - acc: 0.6867\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 0s 132us/step - loss: 0.4731 - acc: 0.6933\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 0s 130us/step - loss: 0.4728 - acc: 0.6933\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 0s 138us/step - loss: 0.4725 - acc: 0.7000\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 0s 123us/step - loss: 0.4723 - acc: 0.7000\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - 0s 146us/step - loss: 0.4719 - acc: 0.7000\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 0s 196us/step - loss: 0.4716 - acc: 0.7000\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 0s 196us/step - loss: 0.4714 - acc: 0.7000\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 0s 161us/step - loss: 0.4711 - acc: 0.7000\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - 0s 159us/step - loss: 0.4708 - acc: 0.7000\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 0s 130us/step - loss: 0.4705 - acc: 0.7000\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 0s 150us/step - loss: 0.4702 - acc: 0.7067\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 0s 168us/step - loss: 0.4699 - acc: 0.7200\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 0s 186us/step - loss: 0.4697 - acc: 0.7200\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 0s 143us/step - loss: 0.4693 - acc: 0.7200\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 0s 182us/step - loss: 0.4691 - acc: 0.7267\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 0s 142us/step - loss: 0.4687 - acc: 0.7200\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 0s 204us/step - loss: 0.4684 - acc: 0.7400\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 0s 144us/step - loss: 0.4681 - acc: 0.7400\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 0s 134us/step - loss: 0.4677 - acc: 0.7467\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 0s 128us/step - loss: 0.4674 - acc: 0.7467\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - 0s 162us/step - loss: 0.4671 - acc: 0.7467\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 0s 149us/step - loss: 0.4667 - acc: 0.7533\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 0s 155us/step - loss: 0.4664 - acc: 0.7533\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 0s 155us/step - loss: 0.4661 - acc: 0.7533\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 0s 139us/step - loss: 0.4657 - acc: 0.7467\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 0s 172us/step - loss: 0.4654 - acc: 0.7600\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 0s 179us/step - loss: 0.4650 - acc: 0.7600\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.4646 - acc: 0.7600\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 0s 172us/step - loss: 0.4641 - acc: 0.7667\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 0s 144us/step - loss: 0.4638 - acc: 0.7600\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 0s 167us/step - loss: 0.4633 - acc: 0.7667\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 0s 141us/step - loss: 0.4630 - acc: 0.7667\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 0s 149us/step - loss: 0.4625 - acc: 0.7667\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 0s 154us/step - loss: 0.4620 - acc: 0.7800\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 0s 184us/step - loss: 0.4616 - acc: 0.7800\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.4611 - acc: 0.7800\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 0s 133us/step - loss: 0.4605 - acc: 0.7867\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 0s 174us/step - loss: 0.4601 - acc: 0.7933\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 0s 189us/step - loss: 0.4595 - acc: 0.7933\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 0s 146us/step - loss: 0.4590 - acc: 0.7933\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 0s 146us/step - loss: 0.4584 - acc: 0.7933\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 0s 137us/step - loss: 0.4579 - acc: 0.7933\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 0s 126us/step - loss: 0.4573 - acc: 0.7667\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 0s 155us/step - loss: 0.4566 - acc: 0.7733\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 0s 165us/step - loss: 0.4560 - acc: 0.7667\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - 0s 151us/step - loss: 0.4551 - acc: 0.7667\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 0s 168us/step - loss: 0.4540 - acc: 0.7667\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 0s 136us/step - loss: 0.4526 - acc: 0.8133\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 0s 135us/step - loss: 0.4518 - acc: 0.8400\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 0s 147us/step - loss: 0.4507 - acc: 0.8400\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 0s 110us/step - loss: 0.4494 - acc: 0.8400\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 0s 152us/step - loss: 0.4482 - acc: 0.8133\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 0s 184us/step - loss: 0.4467 - acc: 0.7800\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 0s 217us/step - loss: 0.4446 - acc: 0.7933\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 0s 173us/step - loss: 0.4417 - acc: 0.8133\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.4972 - acc: 0.750 - 0s 170us/step - loss: 0.4384 - acc: 0.8200\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 0s 164us/step - loss: 0.4343 - acc: 0.8667\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 0s 125us/step - loss: 0.4288 - acc: 0.8867\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 0s 141us/step - loss: 0.4235 - acc: 0.8667\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 0s 182us/step - loss: 0.4180 - acc: 0.8667\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.4116 - acc: 0.8667\n",
      "Epoch 230/300\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.4062 - acc: 0.8667\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 0s 147us/step - loss: 0.4006 - acc: 0.8733\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 0s 159us/step - loss: 0.3949 - acc: 0.8867\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 0s 137us/step - loss: 0.3895 - acc: 0.8867\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 0s 139us/step - loss: 0.3835 - acc: 0.8867\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 0s 162us/step - loss: 0.3776 - acc: 0.8933\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 0s 160us/step - loss: 0.3713 - acc: 0.8933\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - 0s 122us/step - loss: 0.3649 - acc: 0.8733\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 0s 166us/step - loss: 0.3584 - acc: 0.8733\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.3515 - acc: 0.8933\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 0s 124us/step - loss: 0.3449 - acc: 0.8933\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 0s 108us/step - loss: 0.3379 - acc: 0.9000\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 0s 101us/step - loss: 0.3310 - acc: 0.9000\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 0s 104us/step - loss: 0.3257 - acc: 0.9000\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 0s 97us/step - loss: 0.3186 - acc: 0.8933\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.3127 - acc: 0.9133\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - 0s 103us/step - loss: 0.3069 - acc: 0.9067\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 0s 98us/step - loss: 0.2994 - acc: 0.9000\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 0s 97us/step - loss: 0.2950 - acc: 0.9000\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 0s 107us/step - loss: 0.2937 - acc: 0.8867\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 0s 100us/step - loss: 0.2821 - acc: 0.9133\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 0s 98us/step - loss: 0.2769 - acc: 0.9133\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 0s 103us/step - loss: 0.2723 - acc: 0.9067\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 0s 95us/step - loss: 0.2660 - acc: 0.9000\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 0s 95us/step - loss: 0.2639 - acc: 0.9067\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 0s 104us/step - loss: 0.2564 - acc: 0.9133\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 0s 106us/step - loss: 0.2528 - acc: 0.9133\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 0s 94us/step - loss: 0.2495 - acc: 0.9200\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 0s 100us/step - loss: 0.2452 - acc: 0.9133\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 0s 102us/step - loss: 0.2398 - acc: 0.9200\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 0s 103us/step - loss: 0.2345 - acc: 0.9267\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 0s 97us/step - loss: 0.2312 - acc: 0.9333\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 0s 109us/step - loss: 0.2273 - acc: 0.9267\n",
      "Epoch 263/300\n",
      "150/150 [==============================] - 0s 96us/step - loss: 0.2242 - acc: 0.9333\n",
      "Epoch 264/300\n",
      "150/150 [==============================] - 0s 105us/step - loss: 0.2197 - acc: 0.9267\n",
      "Epoch 265/300\n",
      "150/150 [==============================] - 0s 97us/step - loss: 0.2161 - acc: 0.9400\n",
      "Epoch 266/300\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.2124 - acc: 0.9400\n",
      "Epoch 267/300\n",
      "150/150 [==============================] - 0s 100us/step - loss: 0.2089 - acc: 0.9400\n",
      "Epoch 268/300\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.2058 - acc: 0.9400\n",
      "Epoch 269/300\n",
      "150/150 [==============================] - 0s 104us/step - loss: 0.2025 - acc: 0.9400\n",
      "Epoch 270/300\n",
      "150/150 [==============================] - 0s 107us/step - loss: 0.1993 - acc: 0.9467\n",
      "Epoch 271/300\n",
      "150/150 [==============================] - 0s 118us/step - loss: 0.1965 - acc: 0.9400\n",
      "Epoch 272/300\n",
      "150/150 [==============================] - 0s 101us/step - loss: 0.1924 - acc: 0.9467\n",
      "Epoch 273/300\n",
      "150/150 [==============================] - 0s 95us/step - loss: 0.1903 - acc: 0.9400\n",
      "Epoch 274/300\n",
      "150/150 [==============================] - 0s 95us/step - loss: 0.1857 - acc: 0.9400\n",
      "Epoch 275/300\n",
      "150/150 [==============================] - 0s 101us/step - loss: 0.1844 - acc: 0.9467\n",
      "Epoch 276/300\n",
      "150/150 [==============================] - 0s 95us/step - loss: 0.1822 - acc: 0.9333\n",
      "Epoch 277/300\n",
      "150/150 [==============================] - 0s 96us/step - loss: 0.1779 - acc: 0.9467\n",
      "Epoch 278/300\n",
      "150/150 [==============================] - 0s 97us/step - loss: 0.1759 - acc: 0.9467\n",
      "Epoch 279/300\n",
      "150/150 [==============================] - 0s 96us/step - loss: 0.1731 - acc: 0.9400\n",
      "Epoch 280/300\n",
      "150/150 [==============================] - 0s 103us/step - loss: 0.1694 - acc: 0.9467\n",
      "Epoch 281/300\n",
      "150/150 [==============================] - 0s 96us/step - loss: 0.1673 - acc: 0.9533\n",
      "Epoch 282/300\n",
      "150/150 [==============================] - 0s 100us/step - loss: 0.1641 - acc: 0.9533\n",
      "Epoch 283/300\n",
      "150/150 [==============================] - 0s 96us/step - loss: 0.1635 - acc: 0.9467\n",
      "Epoch 284/300\n",
      "150/150 [==============================] - 0s 97us/step - loss: 0.1595 - acc: 0.9467\n",
      "Epoch 285/300\n",
      "150/150 [==============================] - 0s 105us/step - loss: 0.1573 - acc: 0.9533\n",
      "Epoch 286/300\n",
      "150/150 [==============================] - 0s 102us/step - loss: 0.1557 - acc: 0.9600\n",
      "Epoch 287/300\n",
      "150/150 [==============================] - 0s 106us/step - loss: 0.1534 - acc: 0.9533\n",
      "Epoch 288/300\n",
      "150/150 [==============================] - 0s 105us/step - loss: 0.1509 - acc: 0.9533\n",
      "Epoch 289/300\n",
      "150/150 [==============================] - 0s 95us/step - loss: 0.1496 - acc: 0.9600\n",
      "Epoch 290/300\n",
      "150/150 [==============================] - 0s 100us/step - loss: 0.1480 - acc: 0.9467\n",
      "Epoch 291/300\n",
      "150/150 [==============================] - 0s 107us/step - loss: 0.1452 - acc: 0.9467\n",
      "Epoch 292/300\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.1422 - acc: 0.9600\n",
      "Epoch 293/300\n",
      "150/150 [==============================] - 0s 99us/step - loss: 0.1405 - acc: 0.9600\n",
      "Epoch 294/300\n",
      "150/150 [==============================] - 0s 103us/step - loss: 0.1392 - acc: 0.9533\n",
      "Epoch 295/300\n",
      "150/150 [==============================] - 0s 109us/step - loss: 0.1369 - acc: 0.9467\n",
      "Epoch 296/300\n",
      "150/150 [==============================] - 0s 102us/step - loss: 0.1353 - acc: 0.9600\n",
      "Epoch 297/300\n",
      "150/150 [==============================] - 0s 104us/step - loss: 0.1349 - acc: 0.9600\n",
      "Epoch 298/300\n",
      "150/150 [==============================] - 0s 109us/step - loss: 0.1330 - acc: 0.9533\n",
      "Epoch 299/300\n",
      "150/150 [==============================] - 0s 110us/step - loss: 0.1296 - acc: 0.9600\n",
      "Epoch 300/300\n",
      "150/150 [==============================] - 0s 104us/step - loss: 0.1289 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8cb3dca50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X, y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.save(\"final_iris_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_scaler.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, 'iris_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "flower_model = load_model('final_iris_model.h5')\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_example = {\"sepal_length\": 5.1,\n",
    "                  \"sepal_width\": 3.5,\n",
    "                  \"petal_length\": 1.4,\n",
    "                  \"petal_width\": 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(model, scaler, sample_json):\n",
    "    \n",
    "    \n",
    "    \n",
    "    s_len = sample_json[\"sepal_length\"]\n",
    "    s_wid = sample_json[\"sepal_width\"]\n",
    "    p_len = sample_json[\"petal_length\"]\n",
    "    p_wid = sample_json[\"petal_width\"]\n",
    "    \n",
    "    flower = [[s_len, s_wid, p_len, p_wid]]\n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)[0]\n",
    "    \n",
    "    return classes[class_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_prediction(flower_model, flower_scaler, flower_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "flower_model = load_model('final_iris_model.h5')\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")\n",
    "\n",
    "def return_prediction(model, scaler, sample_json):\n",
    "    \n",
    "    \n",
    "    \n",
    "    s_len = sample_json[\"sepal_length\"]\n",
    "    s_wid = sample_json[\"sepal_width\"]\n",
    "    p_len = sample_json[\"petal_length\"]\n",
    "    p_wid = sample_json[\"petal_width\"]\n",
    "    \n",
    "    flower = [[s_len, s_wid, p_len, p_wid]]\n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)[0]\n",
    "    \n",
    "    return classes[class_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
